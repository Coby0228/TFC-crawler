# Claude 4 rewrite å¾Œæ²’è·‘é
import requests
from bs4 import BeautifulSoup
import time
import json
import urllib3
import random
import os

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def extract_valid_links(soup):
    """å¾ç¶²é ä¸­æå–æœ‰æ•ˆçš„æŸ¥æ ¸å ±å‘Šé€£çµ"""
    raw_links = soup.select("div.entry-content-wrap a")
    valid_links = []

    for a in raw_links:
        href = a.get("href")
        if href and "/fact-check-reports/" in href and not (
            "report-type" in href or "report-classification" in href
        ):
            valid_links.append(href)

    return valid_links

def crawl_links():
    """çˆ¬å–æ‰€æœ‰æŸ¥æ ¸å ±å‘Šé€£çµ"""
    base_url = "https://tfc-taiwan.org.tw/fact-check-reports-all/?pg={}"
    page = 1
    all_links = set()

    while True:
        url = base_url.format(page)
        print(f"ğŸ“„ æŠ“å–ç¬¬ {page} é : {url}")
        try:
            res = requests.get(url, verify=False, timeout=10)
            res.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"âŒ éŒ¯èª¤ï¼Œç¬¬ {page} é ç„¡æ³•æŠ“å–ï¼š{e}")
            break

        soup = BeautifulSoup(res.text, "html.parser")
        links = extract_valid_links(soup)

        if not links:
            print("âœ… æ²’æœ‰æ›´å¤šè³‡æ–™ï¼ŒçµæŸç¿»é ã€‚")
            break

        all_links.update(links)
        page += 1
        time.sleep(random.randint(1, 6))

    return list(all_links)

def classify_urls(urls):
    """å°‡ URL åˆ†é¡ç‚º migration å’Œå…¶ä»–é¡åˆ¥"""
    migration_urls = []
    other_urls = []

    for url in urls:
        if "migration" in url.lower():
            migration_urls.append(url)
        else:
            other_urls.append(url)
    
    return migration_urls, other_urls

def create_directories():
    """å»ºç«‹å¿…è¦çš„ç›®éŒ„çµæ§‹"""
    directories = ["data", "data/url"]
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"ğŸ“ å»ºç«‹ç›®éŒ„: {directory}")

def save_json_file(data, filepath, description):
    """å„²å­˜ JSON æª”æ¡ˆ"""
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ {description}: {len(data)} ç­†è³‡æ–™å„²å­˜è‡³ {filepath}")

if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹åŸ·è¡Œ TFC æŸ¥æ ¸å ±å‘Šçˆ¬èŸ²èˆ‡åˆ†é¡ç¨‹å¼")
    print("=" * 50)
    
    # æ­¥é©Ÿ 1: å»ºç«‹ç›®éŒ„
    create_directories()
    
    # æ­¥é©Ÿ 2: çˆ¬å–æ‰€æœ‰é€£çµ
    print("\nğŸ“¡ é–‹å§‹çˆ¬å–æŸ¥æ ¸å ±å‘Šé€£çµ...")
    report_links = crawl_links()
    
    # æ­¥é©Ÿ 3: å„²å­˜æ‰€æœ‰é€£çµ
    all_links_file = "data/url/TFC_factcheck_links.json"
    save_json_file(report_links, all_links_file, "æ‰€æœ‰æŸ¥æ ¸å ±å‘Šé€£çµ")
    
    # æ­¥é©Ÿ 4: åˆ†é¡é€£çµ
    print("\nğŸ” é–‹å§‹åˆ†é¡é€£çµ...")
    migration_urls, other_urls = classify_urls(report_links)
    
    # æ­¥é©Ÿ 5: å„²å­˜åˆ†é¡çµæœ
    migration_file = "data/url/migration.json"
    other_file = "data/url/title.json"
    
    save_json_file(migration_urls, migration_file, "Migration ç›¸é—œé€£çµ")
    save_json_file(other_urls, other_file, "å…¶ä»–é€£çµ")
    
    print("\n" + "=" * 50)
    print("âœ… ç¨‹å¼åŸ·è¡Œå®Œæˆï¼")
    print(f"ğŸ“Š ç¸½å…±è™•ç† {len(report_links)} ç­†é€£çµ")
    print(f"   - Migration ç›¸é—œ: {len(migration_urls)} ç­†")
    print(f"   - å…¶ä»–é¡åˆ¥: {len(other_urls)} ç­†")