import json
import requests
import time
import random
from bs4 import BeautifulSoup
from tqdm import tqdm
import urllib3
import re
import os

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Checkpoint è¨­å®š
CHECKPOINT_FILE = "data/checkpoint/scraper_checkpoint.json"
OUTPUT_FILE = "data/raw/TFC_migration_data.json"
CHECKPOINT_INTERVAL = 500  # æ¯è™•ç† 500 å€‹ç¶²å€å°±å„²å­˜ä¸€æ¬¡ checkpoint

def load_checkpoint():
    """è¼‰å…¥ checkpoint æª”æ¡ˆ"""
    if os.path.exists(CHECKPOINT_FILE):
        try:
            with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
                checkpoint = json.load(f)
            print(f"ğŸ“ è¼‰å…¥ checkpoint: å·²å®Œæˆ {checkpoint['completed_count']} å€‹ç¶²å€")
            return checkpoint
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥ checkpoint å¤±æ•—: {e}")
    return {"completed_urls": [], "results": [], "completed_count": 0}

def save_checkpoint(checkpoint):
    """å„²å­˜ checkpoint æª”æ¡ˆ"""
    try:
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(CHECKPOINT_FILE), exist_ok=True)
        
        with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ å„²å­˜ checkpoint å¤±æ•—: {e}")

def save_final_results(results):
    """å„²å­˜æœ€çµ‚çµæœ"""
    try:
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
        
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        print(f"ğŸ’¾ çµæœå·²å„²å­˜åˆ°: {OUTPUT_FILE}")
    except Exception as e:
        print(f"âŒ å„²å­˜çµæœå¤±æ•—: {e}")

def extract_claim_from_title(soup):
    """å¾ HTML <title> æ“·å– claimï¼Œå»æ‰å‰ç¶´ç¬¦è™Ÿèˆ‡ç©ºæ ¼"""
    title_tag = soup.title
    if not title_tag or not title_tag.string:
        return None

    title_text = title_tag.string.strip()

    # ä½¿ç”¨æ­£å‰‡è™•ç†æ ¼å¼
    if 'ã€‘' in title_text:
        pattern = r'ã€‘([^ ]+)'  # ã€‘ å¾Œåˆ°ç¬¬ä¸€å€‹ç©ºç™½
    else:
        pattern = r'^([^ ]+)'  # å¾é–‹é ­åˆ°ç¬¬ä¸€å€‹ç©ºç™½

    match = re.search(pattern, title_text)
    return match.group(1).strip() if match else title_text

def extract_type_from_title(soup):
    """å¾ HTML <title> æ“·å– typeï¼Œå³ã€...ã€‘ä¸­çš„æ–‡å­—"""
    title_tag = soup.title
    if not title_tag or not title_tag.string:
        return None
    title_text = title_tag.string.strip()
    match = re.search(r'ã€(.*?)ã€‘', title_text)
    return match.group(1).strip() if match else None

def extract_fact_check_blocks(soup):
    """
    å¾ HTML æå–æŸ¥æ ¸å ±å‘Šæ­£æ–‡ï¼Œä¸¦å°‡æ‰€æœ‰æ®µè½åˆä½µç‚ºå–®ä¸€å­—ä¸²ã€‚
    æ­¤å‡½å¼é¦–å…ˆå®šä½åˆ° id ç‚º 'kanban' çš„ div å…ƒç´ ï¼Œç„¶å¾Œå°‹æ‰¾å…¶å¾Œç·Šé„°çš„å…„å¼Ÿ div å…ƒç´ ï¼Œ
    é€™å€‹å…„å¼Ÿå…ƒç´ åŒ…å«äº†æ‰€éœ€çš„æŸ¥æ ¸å ±å‘Šå…§å®¹ã€‚
    """
    # å°‹æ‰¾ id="kanban" çš„ div
    kanban_div = soup.find("div", id="kanban")
    
    if kanban_div:
        report_div = kanban_div.find_next_sibling("div")
        if report_div:
            paragraphs = report_div.find_all("p")
            report_texts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            return "\n".join(report_texts)

    return None

def main():
    # è®€å–ç¶²å€æ¸…å–®
    try:
        with open("data/url/migration.json", "r", encoding="utf-8") as f:
            urls = json.load(f)
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ°ç¶²å€æª”æ¡ˆ: data/url/migration.json")
        return
    except Exception as e:
        print(f"âŒ è®€å–ç¶²å€æª”æ¡ˆå¤±æ•—: {e}")
        return

    # è¼‰å…¥ checkpoint
    checkpoint = load_checkpoint()
    completed_urls = set(checkpoint["completed_urls"])
    results = checkpoint["results"]
    
    # éæ¿¾å‡ºå°šæœªè™•ç†çš„ç¶²å€
    remaining_urls = [url for url in urls if url not in completed_urls]
    
    if not remaining_urls:
        print("ğŸ‰ æ‰€æœ‰ç¶²å€éƒ½å·²è™•ç†å®Œç•¢ï¼")
        return
    
    print(f"ğŸ“Š ç¸½å…± {len(urls)} å€‹ç¶²å€ï¼Œå·²å®Œæˆ {len(completed_urls)} å€‹ï¼Œå‰©é¤˜ {len(remaining_urls)} å€‹")

    # é–‹å§‹è™•ç†å‰©é¤˜çš„ç¶²å€
    for i, url in enumerate(tqdm(remaining_urls, desc="æŠ“å–é€²åº¦")):
        try:
            res = requests.get(url, verify=False, timeout=20)
            res.raise_for_status()

            soup = BeautifulSoup(res.text, "html.parser")
            claim_text = extract_claim_from_title(soup)
            type_text = extract_type_from_title(soup)
            report_text = extract_fact_check_blocks(soup)

            result_entry = {
                "url": url,
                "claim": claim_text,
                "report": report_text,
                "type": type_text,
            }
            results.append(result_entry)
            completed_urls.add(url)

            # æ¯éš”ä¸€å®šé–“éš”å„²å­˜ checkpoint
            if (i + 1) % CHECKPOINT_INTERVAL == 0:
                checkpoint_data = {
                    "completed_urls": list(completed_urls),
                    "results": results,
                    "completed_count": len(completed_urls)
                }
                save_checkpoint(checkpoint_data)
                print(f"ğŸ’¾ Checkpoint å·²å„²å­˜ ({len(completed_urls)}/{len(urls)})")

            time.sleep(random.uniform(1, 6))

        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {url} â†’ {e}")
            error_entry = {"url": url, "error": str(e)}
            results.append(error_entry)
            completed_urls.add(url)  # å³ä½¿å‡ºéŒ¯ä¹Ÿæ¨™è¨˜ç‚ºå·²è™•ç†ï¼Œé¿å…é‡è¤‡å˜—è©¦

    # æœ€çµ‚å„²å­˜
    final_checkpoint = {
        "completed_urls": list(completed_urls),
        "results": results,
        "completed_count": len(completed_urls)
    }
    save_checkpoint(final_checkpoint)
    save_final_results(results)

    print("âœ… æ‰€æœ‰é é¢å·²è™•ç†å®Œç•¢")

if __name__ == "__main__":
    main()